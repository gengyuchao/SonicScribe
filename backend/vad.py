import torch
import numpy as np
import torchaudio
from silero_vad import load_silero_vad, get_speech_timestamps, VADIterator, read_audio

class VADProcessor:
    def __init__(self, threshold=0.5, sampling_rate=16000):
        """
        åˆå§‹åŒ– VAD å¤„ç†å™¨
        :param threshold: VAD é˜ˆå€¼ (0.0-1.0)
        :param sampling_rate: é‡‡æ ·ç‡ (Hz)ï¼Œå¿…é¡»æ˜¯8000æˆ–16000
        """
        self.model = load_silero_vad()
        self.sampling_rate = sampling_rate
        self.threshold = threshold
        self.min_speech_duration = 0.3  # æœ€å°è¯­éŸ³æŒç»­æ—¶é—´(ç§’)
        self.max_silence_duration = 1.0  # æœ€å¤§é™éŸ³æŒç»­æ—¶é—´(ç§’)
        self.vad_iterator = None
        
        # éªŒè¯é‡‡æ ·ç‡
        if self.sampling_rate not in [8000, 16000]:
            raise ValueError("é‡‡æ ·ç‡å¿…é¡»æ˜¯8000æˆ–16000 Hz")
    
    def _normalize_audio(self, audio_tensor):
        """
        æ ‡å‡†åŒ–éŸ³é¢‘åˆ°[-1, 1]èŒƒå›´
        """
        if isinstance(audio_tensor, np.ndarray):
            audio_tensor = torch.tensor(audio_tensor, dtype=torch.float32)
        
        # ç¡®ä¿æ˜¯æµ®ç‚¹ç±»å‹
        if audio_tensor.dtype not in [torch.float32, torch.float64]:
            audio_tensor = audio_tensor.float()
        
        # å½’ä¸€åŒ–åˆ°[-1, 1]èŒƒå›´
        if audio_tensor.abs().max() > 1.0:
            audio_tensor = audio_tensor / audio_tensor.abs().max()
        
        return audio_tensor
    
    def detect_voice_activity(self, audio_tensor, threshold=None):
        """
        æ£€æµ‹è¯­éŸ³æ´»åŠ¨
        :param audio_tensor: éŸ³é¢‘å¼ é‡ï¼Œå½¢çŠ¶ä¸º [1, T] æˆ– [T]
        :param threshold: VADé˜ˆå€¼ (å¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–å€¼)
        :return: (speech_timestamps, is_speech)
        """
        if threshold is None:
            threshold = self.threshold
        
        # ç¡®ä¿éŸ³é¢‘æ˜¯æ­£ç¡®çš„æ ¼å¼
        if isinstance(audio_tensor, torch.Tensor):
            audio = audio_tensor.squeeze()
        else:
            audio = torch.tensor(audio_tensor).squeeze()
        
        # æ ‡å‡†åŒ–éŸ³é¢‘
        audio = self._normalize_audio(audio)
        
        # ç¡®ä¿æ˜¯16kHzé‡‡æ ·ç‡ï¼ˆåˆ›å»ºå‰¯æœ¬ï¼Œä¸ä¿®æ”¹self.sampling_rateï¼‰
        current_sampling_rate = self.sampling_rate
        if current_sampling_rate != 16000:
            resampler = torchaudio.transforms.Resample(
                orig_freq=current_sampling_rate, 
                new_freq=16000
            )
            audio = resampler(audio)
            current_sampling_rate = 16000
        
        # ä½¿ç”¨æ­£ç¡®çš„ API è°ƒç”¨
        speech_timestamps = get_speech_timestamps(
            audio,
            self.model,
            threshold=threshold,
            sampling_rate=current_sampling_rate,
            min_speech_duration_ms=int(self.min_speech_duration * 1000),
            max_speech_duration_s=float('inf'),
            min_silence_duration_ms=int(self.max_silence_duration * 1000)
        )
        
        is_speech = len(speech_timestamps) > 0
        return speech_timestamps, is_speech
    
    def is_voice_active(self, audio_chunk, threshold=None):
        """
        æ£€æŸ¥éŸ³é¢‘å—æ˜¯å¦åŒ…å«è¯­éŸ³
        :param audio_chunk: éŸ³é¢‘å—ï¼Œnumpy æ•°ç»„æˆ– torch å¼ é‡
        :param threshold: VADé˜ˆå€¼ (å¯é€‰ï¼Œè¦†ç›–åˆå§‹åŒ–å€¼)
        :return: bool
        """
        if threshold is None:
            threshold = self.threshold
        
        # ç¡®ä¿éŸ³é¢‘æ˜¯æ­£ç¡®çš„æ ¼å¼
        if isinstance(audio_chunk, np.ndarray):
            audio_tensor = torch.tensor(audio_chunk, dtype=torch.float32)
        elif isinstance(audio_chunk, torch.Tensor):
            audio_tensor = audio_chunk.float()
        else:
            raise ValueError("éŸ³é¢‘å—å¿…é¡»æ˜¯ numpy æ•°ç»„æˆ– torch å¼ é‡")
        
        # æ ‡å‡†åŒ–éŸ³é¢‘
        audio_tensor = self._normalize_audio(audio_tensor)
        
        # ç¡®ä¿æ˜¯16kHzé‡‡æ ·ç‡
        current_sampling_rate = self.sampling_rate
        if current_sampling_rate != 16000:
            resampler = torchaudio.transforms.Resample(
                orig_freq=current_sampling_rate, 
                new_freq=16000
            )
            audio_tensor = resampler(audio_tensor)
            current_sampling_rate = 16000
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¯­éŸ³
        speech_timestamps = get_speech_timestamps(
            audio_tensor,
            self.model,
            threshold=threshold,
            sampling_rate=current_sampling_rate,
            min_speech_duration_ms=100,  # 100ms æœ€å°è¯­éŸ³æŒç»­æ—¶é—´
            max_speech_duration_s=1.0,   # 1ç§’æœ€å¤§è¯­éŸ³æŒç»­æ—¶é—´
            min_silence_duration_ms=100  # 100ms æœ€å°é™éŸ³æŒç»­æ—¶é—´
        )
        
        return len(speech_timestamps) > 0
    
    def set_threshold(self, threshold):
        """
        åŠ¨æ€è®¾ç½® VAD é˜ˆå€¼
        :param threshold: æ–°çš„é˜ˆå€¼ (0.0-1.0)
        """
        if not 0.0 <= threshold <= 1.0:
            raise ValueError("é˜ˆå€¼å¿…é¡»åœ¨ 0.0 åˆ° 1.0 ä¹‹é—´")
        self.threshold = threshold
        print(f"âœ… VAD é˜ˆå€¼æ›´æ–°ä¸º: {threshold}")
    
    def get_threshold(self):
        """
        è·å–å½“å‰ VAD é˜ˆå€¼
        :return: float
        """
        return self.threshold
    
    def reset(self):
        """é‡ç½® VAD è¿­ä»£å™¨"""
        self.vad_iterator = None
    
    def load_audio_file(self, file_path, target_sampling_rate=16000):
        """
        åŠ è½½å¹¶é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶
        :param file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        :param target_sampling_rate: ç›®æ ‡é‡‡æ ·ç‡
        :return: å¤„ç†åçš„éŸ³é¢‘å¼ é‡
        """
        try:
            # å°è¯•ä½¿ç”¨torchaudioåŠ è½½
            waveform, sample_rate = torchaudio.load(file_path)
            print(f"ğŸµ ä½¿ç”¨ torchaudio åŠ è½½éŸ³é¢‘æ–‡ä»¶: {file_path}")
            print(f"   åŸå§‹é‡‡æ ·ç‡: {sample_rate}Hz, å½¢çŠ¶: {waveform.shape}")
        except Exception as e:
            print(f"âš ï¸ torchaudioåŠ è½½å¤±è´¥: {e}")
            try:
                # å°è¯•ä½¿ç”¨librosaåŠ è½½
                import librosa
                waveform, sample_rate = librosa.load(file_path, sr=None, mono=True)
                waveform = torch.tensor(waveform).unsqueeze(0)
                print(f"ğŸµ ä½¿ç”¨ librosa åŠ è½½éŸ³é¢‘æ–‡ä»¶: {file_path}")
                print(f"   åŸå§‹é‡‡æ ·ç‡: {sample_rate}Hz, å½¢çŠ¶: {waveform.shape}")
            except Exception as e:
                print(f"âš ï¸ librosaåŠ è½½å¤±è´¥: {e}")
                # ä½¿ç”¨silero_vadçš„read_audioä½œä¸ºæœ€åæ‰‹æ®µ
                waveform = read_audio(file_path, sampling_rate=target_sampling_rate)
                sample_rate = target_sampling_rate
                print(f"ğŸµ ä½¿ç”¨ silero_vad read_audio åŠ è½½éŸ³é¢‘æ–‡ä»¶: {file_path}")
        
        # ç¡®ä¿æ˜¯å•å£°é“
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
            print("   â¡ï¸ è½¬æ¢ä¸ºå•å£°é“")
        
        # é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
        if sample_rate != target_sampling_rate:
            resampler = torchaudio.transforms.Resample(
                orig_freq=sample_rate,
                new_freq=target_sampling_rate
            )
            waveform = resampler(waveform)
            print(f"   â¡ï¸ é‡é‡‡æ ·åˆ° {target_sampling_rate}Hz")
        
        # æ ‡å‡†åŒ–éŸ³é¢‘
        waveform = self._normalize_audio(waveform)
        
        return waveform.squeeze()