import os
import tempfile
from pathlib import Path
from typing import Union, Dict, Any, List, Optional
import numpy as np
import soundfile as sf
import torch
import torchaudio
from transformers import (
    AutoConfig,
    AutoModel,
    AutoProcessor,
)
import warnings

try:
    import bitsandbytes as bnb
    from bitsandbytes.nn import Int8Params
    HAS_BITSANDBYTES = True
except ImportError:
    HAS_BITSANDBYTES = False
    warnings.warn("bitsandbytes not installed. INT8 mode will not be available. Install with: pip install bitsandbytes")

class ASRModel:
    def __init__(self, checkpoint_dir: str, device: str = "cuda", mode: str = "native"):
        """
        åˆå§‹åŒ– ASR æ¨¡å‹ï¼Œæ”¯æŒåŸç”Ÿæ¨¡å¼å’Œ INT8 é‡åŒ–æ¨¡å¼ã€‚
        
        Args:
            checkpoint_dir: æ¨¡å‹æ£€æŸ¥ç‚¹ç›®å½•è·¯å¾„ã€‚
            device: è¿è¡Œè®¾å¤‡ ("cuda" æˆ– "cpu")ã€‚
            mode: è¿è¡Œæ¨¡å¼ï¼Œå¯é€‰ "native" (åŸç”Ÿ bfloat16) æˆ– "int8" (8-bit é‡åŒ–)
                - "native": ä½¿ç”¨ torch.bfloat16ï¼Œç²¾åº¦é«˜ï¼Œæ˜¾å­˜å ç”¨å¤§
                - "int8": ä½¿ç”¨ 8-bit é‡åŒ–ï¼Œæ˜¾å­˜å ç”¨å°ï¼Œé€‚åˆ GTX1060 ç­‰å°æ˜¾å­˜æ˜¾å¡
        """
        # éªŒè¯æ¨¡å¼
        if mode not in ["native", "int8"]:
            raise ValueError("mode must be either 'native' or 'int8'")
        
        if mode == "int8" and not HAS_BITSANDBYTES:
            raise ImportError("INT8 mode requires bitsandbytes. Install with: pip install bitsandbytes")
        
        # ç¡®å®šè®¾å¤‡
        self.device = torch.device(device if torch.cuda.is_available() else "cpu")
        self.mode = mode
        
        # è®¾ç½®æ¨¡å‹æ•°æ®ç±»å‹
        self.model_dtype = torch.bfloat16 if mode == "native" else torch.float16
        
        self.checkpoint_dir = Path(checkpoint_dir)
        
        # åŠ è½½ Processor
        self.processor = AutoProcessor.from_pretrained(str(self.checkpoint_dir))
        self.target_sr = self.processor.feature_extractor.sampling_rate
        
        # åŠ è½½é…ç½®
        self.config = AutoConfig.from_pretrained(self.checkpoint_dir, trust_remote_code=True)
        
        print(f"ğŸš€ åˆå§‹åŒ– ASR æ¨¡å‹ | æ¨¡å¼: {mode.upper()} | è®¾å¤‡: {self.device}")

        # æ£€æŸ¥æ˜¯å¦ä¸º GLM-ASR æ¨¡å‹
        self.is_glm_asr = hasattr(self.config, "model_type") and "glm" in str(self.config.model_type).lower()
        print(f"ğŸ” æ£€æµ‹åˆ°æ¨¡å‹ç±»å‹: {'GLM-ASR' if self.is_glm_asr else 'å…¶ä»–æ¨¡å‹'}")

        # åŠ è½½æ¨¡å‹
        if mode == "int8" and self.is_glm_asr:
            self.model = self._load_glm_asr_int8()
        else:
            self.model = self._load_model_standard(mode)
        
        self.model.eval()
        
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        self._print_model_info()

    def _load_model_standard(self, mode: str):
        """æ ‡å‡†æ–¹å¼åŠ è½½æ¨¡å‹ï¼ˆåŸç”Ÿæ¨¡å¼æˆ–éGLM-ASRçš„INT8æ¨¡å¼ï¼‰"""
        model_kwargs = {
            "trust_remote_code": True,
        }
        
        if self.device.type == "cuda":
            model_kwargs["device_map"] = "auto" if mode == "int8" else str(self.device)
        
        if mode == "native":
            model_kwargs["torch_dtype"] = self.model_dtype
        
        if mode == "int8" and not self.is_glm_asr:
            # éGLM-ASRæ¨¡å‹ä½¿ç”¨æ ‡å‡†çš„load_in_8bit
            model_kwargs["load_in_8bit"] = True
        
        # åŠ è½½æ¨¡å‹
        model = AutoModel.from_pretrained(
            self.checkpoint_dir,
            **model_kwargs
        )
        
        # åŸç”Ÿæ¨¡å¼éœ€è¦æ‰‹åŠ¨ç§»åŠ¨åˆ°è®¾å¤‡
        if mode == "native" and self.device.type == "cuda":
            model = model.to(self.device)
        
        return model

    def _load_glm_asr_int8(self):
        """ä¸“é—¨å¤„ç†GLM-ASRæ¨¡å‹çš„8-bité‡åŒ–åŠ è½½"""
        print("ğŸ”§ ä½¿ç”¨æ‰‹åŠ¨é‡åŒ–æ–¹å¼åŠ è½½ GLM-ASR æ¨¡å‹ (INT8 æ¨¡å¼)")
        
        # 1. é¦–å…ˆä»¥float16åŠ è½½æ¨¡å‹åˆ°CPU
        with torch.device('cpu'):
            model = AutoModel.from_pretrained(
                self.checkpoint_dir,
                torch_dtype=torch.float16,
                trust_remote_code=True,
            )
        
        # 2. åº”ç”¨8-bité‡åŒ–
        self._quantize_model_int8(model)
        
        # 3. ç§»åŠ¨åˆ°GPU
        if self.device.type == "cuda":
            model = model.to(self.device)
        
        return model

    def _quantize_model_int8(self, model):
        """æ‰‹åŠ¨å°†æ¨¡å‹è½¬æ¢ä¸º8-bité‡åŒ–"""
        print("âš¡ åº”ç”¨ 8-bit é‡åŒ–åˆ°æ¨¡å‹...")
        
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                # è·³è¿‡ä¸éœ€è¦é‡åŒ–çš„å±‚ï¼ˆå¦‚lm_headï¼‰
                if any(skip_name in name for skip_name in ['lm_head', 'embed_tokens', 'audio_proj']):
                    continue
                
                print(f"  ğŸ“¦ é‡åŒ–çº¿æ€§å±‚: {name}")
                
                # åˆ›å»º8-bitçº¿æ€§å±‚
                quantized_linear = bnb.nn.Linear8bitLt(
                    module.in_features,
                    module.out_features,
                    bias=module.bias is not None,
                    has_fp16_weights=False,  # ä½¿ç”¨çº¯INT8
                    threshold=6.0,  # é»˜è®¤é˜ˆå€¼
                )
                
                # å¤åˆ¶æƒé‡å¹¶é‡åŒ–
                quantized_linear.weight = bnb.nn.Int8Params(
                    module.weight.data.cpu(), 
                    requires_grad=False, 
                    has_fp16_weights=False
                )
                
                if module.bias is not None:
                    quantized_linear.bias = torch.nn.Parameter(module.bias.data.cpu())
                
                # æ›¿æ¢åŸæ¨¡å—
                parent_name = '.'.join(name.split('.')[:-1])
                child_name = name.split('.')[-1]
                
                if parent_name:
                    parent_module = dict(model.named_modules())[parent_name]
                    setattr(parent_module, child_name, quantized_linear)
                else:
                    setattr(model, child_name, quantized_linear)
        
        print("âœ… 8-bit é‡åŒ–å®Œæˆ")

    def _print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯å’Œæ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
        if self.device.type == "cuda":
            torch.cuda.empty_cache()
            allocated = torch.cuda.memory_allocated() / 1024**2
            reserved = torch.cuda.memory_reserved() / 1024**2
            print(f"ğŸ“Š GPU æ˜¾å­˜ä½¿ç”¨: å·²åˆ†é… {allocated:.1f}MB | å·²ä¿ç•™ {reserved:.1f}MB")
        
        # æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°: æ€»è®¡ {total_params/1e9:.2f}B | å¯è®­ç»ƒ {trainable_params/1e9:.2f}B")
        
        if self.mode == "int8":
            print("ğŸ’¡ INT8 æ¨¡å¼æç¤º: æ˜¾å­˜å ç”¨å¤§å¹…é™ä½ï¼Œä½†ç²¾åº¦å¯èƒ½ç•¥æœ‰ä¸‹é™ã€‚é€‚åˆ GTX1060 ç­‰å°æ˜¾å­˜æ˜¾å¡ã€‚")

    def _prepare_audio_tempfile(self, audio_tensor: torch.Tensor, sampling_rate: int) -> str:
        """
        é¢„å¤„ç†éŸ³é¢‘å¼ é‡å¹¶ä¿å­˜åˆ°ä¸´æ—¶ WAV æ–‡ä»¶ã€‚
        
        å¤„ç†æµç¨‹ï¼š
        1. ç¡®ä¿å•å£°é“ã€‚
        2. é‡é‡‡æ ·è‡³ç›®æ ‡é‡‡æ ·ç‡ã€‚
        3. å½’ä¸€åŒ–ã€‚
        4. ä¿å­˜è‡³ä¸´æ—¶æ–‡ä»¶ã€‚
        
        Args:
            audio_tensor: è¾“å…¥éŸ³é¢‘å¼ é‡ (Channel, Time) æˆ–ã€‚
            sampling_rate: åŸå§‹é‡‡æ ·ç‡ã€‚
            
        Returns:
            ä¸´æ—¶æ–‡ä»¶çš„ç»å¯¹è·¯å¾„ã€‚
        """
        # å¤„ç† 1D è¾“å…¥ -> 2D (1, N)
        if audio_tensor.dim() == 1:
            audio_tensor = audio_tensor.unsqueeze(0)
            
        # å–å•å£°é“ (å¦‚æœè¾“å…¥æ˜¯å¤šå£°é“ï¼Œå–ç¬¬ä¸€å£°é“)
        wav = audio_tensor[:1, :]

        # é‡é‡‡æ · (å¦‚æœé‡‡æ ·ç‡ä¸åŒ¹é…)
        if sampling_rate != self.target_sr:
            # æ¯æ¬¡å®ä¾‹åŒ– Resample å¯èƒ½ä¼šæœ‰å¼€é”€ï¼Œä½†èƒ½åŠ¨æ€é€‚åº”ä¸åŒè¾“å…¥é‡‡æ ·ç‡
            resampler = torchaudio.transforms.Resample(
                orig_freq=sampling_rate, 
                new_freq=self.target_sr
            )
            wav = resampler(wav)

        # å½’ä¸€åŒ– (ä¿æŒåŸé€»è¾‘ï¼šé¿å…é™¤é›¶ï¼Œå¹¶å½’ä¸€åŒ–åˆ° [-1, 1])
        # æ³¨æ„ï¼šè¿™ä¼šæ”¹å˜éŸ³é¢‘çš„ç»å¯¹å“åº¦ï¼Œä½†ä¿æŒç›¸å¯¹åŠ¨æ€èŒƒå›´
        max_val = torch.max(torch.abs(wav))
        if max_val > 1e-6:
            wav = wav / max_val
            
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        # ä½¿ç”¨ delete=Falseï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦åœ¨ä¸Šä¸‹æ–‡ä¹‹å¤–ç”± processor è¯»å–å®ƒ
        # æ–‡ä»¶å°†åœ¨ transcribe ç»“æŸæ—¶æ‰‹åŠ¨åˆ é™¤
        fd, tmp_path = tempfile.mkstemp(suffix='.wav')
        os.close(fd) # å…³é—­æ–‡ä»¶æè¿°ç¬¦ï¼Œè®© soundfile å¯ä»¥æ‰“å¼€å®ƒ
        
        # ä¿å­˜éŸ³é¢‘
        sf.write(tmp_path, wav.squeeze(0).cpu().numpy(), self.target_sr)
        
        return tmp_path

    def _prepare_model_inputs(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """
        å‡†å¤‡æ¨¡å‹è¾“å…¥æ•°æ®ï¼Œç¡®ä¿å¼ é‡ä½äºæ­£ç¡®çš„è®¾å¤‡å’Œæ‹¥æœ‰æ­£ç¡®çš„ç²¾åº¦ã€‚
        """
        prepared_inputs = {}
        for key, value in inputs.items():
            if isinstance(value, torch.Tensor):
                # input_ids å’Œ attention_mask å¿…é¡»æ˜¯ long ç±»å‹
                if key in ("input_ids", "attention_mask"):
                    prepared_inputs[key] = value.to(self.device, dtype=torch.long)
                # å…¶ä»–æµ®ç‚¹å¼ é‡è½¬æ¢ä¸ºæ¨¡å‹ç²¾åº¦
                elif value.is_floating_point():
                    if self.mode == "native":
                        prepared_inputs[key] = value.to(self.device, dtype=self.model_dtype)
                    else:
                        # INT8 æ¨¡å¼ä½¿ç”¨ float16
                        prepared_inputs[key] = value.to(self.device, dtype=torch.float16)
                else:
                    prepared_inputs[key] = value.to(self.device)
            else:
                prepared_inputs[key] = value
        return prepared_inputs

    def _format_hotwords_prompt(self, hotwords: List[str], max_hotwords: int = 10) -> str:
        """
        æ ¼å¼åŒ–çƒ­è¯æç¤ºè¯­å¥
        
        Args:
            hotwords: çƒ­è¯åˆ—è¡¨
            max_hotwords: æœ€å¤§çƒ­è¯æ•°é‡é™åˆ¶
            
        Returns:
            æ ¼å¼åŒ–åçš„çƒ­è¯æç¤ºå­—ç¬¦ä¸²
        """
        if not hotwords:
            return ""
        
        # æ¸…ç†å’Œå»é‡çƒ­è¯
        cleaned_hotwords = [
            hw.strip().lower() 
            for hw in set(hotwords) 
            if hw and isinstance(hw, str) and hw.strip()
        ]
        
        if not cleaned_hotwords:
            return ""
        
        # é™åˆ¶çƒ­è¯æ•°é‡
        if len(cleaned_hotwords) > max_hotwords:
            cleaned_hotwords = cleaned_hotwords[:max_hotwords]
        
        # æ„å»ºæç¤ºè¯­å¥
        hotwords_str = ", ".join(f'"{hw}"' for hw in cleaned_hotwords)
        return f". Pay special attention to these important terms: {hotwords_str}"

    def transcribe(
        self, 
        audio_tensor: torch.Tensor, 
        sampling_rate: int = 16000, 
        max_new_tokens: int = 128,
        hotwords: Optional[List[str]] = None,
        return_debug_info: bool = False
    ) -> Union[str, Dict[str, Any]]:
        """
        æ‰§è¡Œè¯­éŸ³è¯†åˆ«è½¬å½•ï¼Œæ”¯æŒçƒ­è¯å¢å¼ºåŠŸèƒ½
        
        Args:
            audio_tensor: è¾“å…¥éŸ³é¢‘å¼ é‡ã€‚
            sampling_rate: éŸ³é¢‘é‡‡æ ·ç‡ã€‚
            max_new_tokens: æœ€å¤§ç”Ÿæˆçš„ token æ•°é‡ã€‚
            hotwords: éœ€è¦ç‰¹åˆ«å…³æ³¨çš„çƒ­è¯åˆ—è¡¨ï¼Œä¾‹å¦‚ ["brand name", "product name"]
            return_debug_info: æ˜¯å¦è¿”å›è°ƒè¯•ä¿¡æ¯ï¼ˆåŒ…æ‹¬å¤„ç†æ—¶é—´å’Œæ˜¾å­˜ä½¿ç”¨ï¼‰
            
        Returns:
            è½¬å½•åçš„æ–‡æœ¬å­—ç¬¦ä¸²ï¼Œæˆ–åŒ…å«è°ƒè¯•ä¿¡æ¯çš„å­—å…¸ï¼ˆå¦‚æœ return_debug_info=Trueï¼‰
        """
        temp_audio_path = None
        start_time = torch.cuda.Event(enable_timing=True) if self.device.type == "cuda" else None
        end_time = torch.cuda.Event(enable_timing=True) if self.device.type == "cuda" else None
        
        try:
            # è®°å½•å¼€å§‹æ—¶é—´
            if self.device.type == "cuda":
                torch.cuda.synchronize()
                start_time = torch.cuda.Event(enable_timing=True)
                end_time = torch.cuda.Event(enable_timing=True)
                start_time.record()
            
            # 1. é¢„å¤„ç†éŸ³é¢‘å¹¶è·å–ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            temp_audio_path = self._prepare_audio_tempfile(audio_tensor, sampling_rate)

            # 2. æ„å»ºåŸºç¡€æŒ‡ä»¤
            base_instruction = "Please transcribe this audio into text"
            
            # 3. æ·»åŠ çƒ­è¯æç¤ºï¼ˆå¦‚æœæä¾›ï¼‰
            hotwords_prompt = self._format_hotwords_prompt(hotwords or [])
            full_instruction = base_instruction + hotwords_prompt

            # 4. æ„å»ºç¬¦åˆ chat template æ ¼å¼çš„æ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "audio", "url": temp_audio_path},
                        {"type": "text", "text": full_instruction},
                    ],
                }
            ]

            # 5. åº”ç”¨ chat template å¹¶è½¬æ¢ä¸ºå¼ é‡
            inputs = self.processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt"
            )

            # 6. è½¬æ¢æ•°æ®ç±»å‹å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = self._prepare_model_inputs(inputs)

            input_length = inputs["input_ids"].shape[1]

            # 7. æ¨ç†ç”Ÿæˆ
            with torch.no_grad():
                if self.mode == "native" and self.device.type == "cuda":
                    # åŸç”Ÿæ¨¡å¼ä½¿ç”¨ autocast ä¼˜åŒ–æ€§èƒ½
                    with torch.autocast(device_type='cuda', dtype=self.model_dtype):
                        outputs = self.model.generate(
                            **inputs,
                            max_new_tokens=max_new_tokens,
                            do_sample=False,
                        )
                else:
                    # INT8 æ¨¡å¼æˆ– CPU æ¨¡å¼ç›´æ¥æ¨ç†
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_new_tokens,
                        do_sample=False,
                    )

            # 8. è§£ç ç»“æœ
            generated_tokens = outputs[:, input_length:]
            transcript = self.processor.batch_decode(
                generated_tokens,
                skip_special_tokens=True
            )[0].strip()

            # è®°å½•ç»“æŸæ—¶é—´
            elapsed_time = 0.0
            if self.device.type == "cuda":
                end_time.record()
                torch.cuda.synchronize()
                elapsed_time = start_time.elapsed_time(end_time) / 1000.0  # è½¬æ¢ä¸ºç§’

            # 9. æ¸…ç†ç¼“å­˜
            if self.device.type == "cuda":
                torch.cuda.empty_cache()

            if return_debug_info:
                debug_info = {
                    "transcript": transcript,
                    "processing_time": elapsed_time,
                    "audio_length_sec": audio_tensor.shape[-1] / sampling_rate,
                    "mode": self.mode,
                    "device": str(self.device),
                }
                
                if self.device.type == "cuda":
                    debug_info.update({
                        "gpu_memory_allocated_mb": torch.cuda.memory_allocated() / 1024**2,
                        "gpu_memory_reserved_mb": torch.cuda.memory_reserved() / 1024**2,
                    })
                
                return debug_info
            
            return transcript

        except RuntimeError as e:
            error_msg = str(e)
            if "out of memory" in error_msg.lower():
                print("âš ï¸ æ˜¾å­˜ä¸è¶³ï¼å»ºè®®ï¼š")
                print("   1. ä½¿ç”¨æ›´çŸ­çš„éŸ³é¢‘")
                print("   2. å‡å°‘ max_new_tokens")
                print("   3. å¦‚æœä½¿ç”¨åŸç”Ÿæ¨¡å¼ï¼Œåˆ‡æ¢åˆ° INT8 æ¨¡å¼")
            elif "load_in_8bit" in error_msg:
                print("âš ï¸ æ¨¡å‹ä¸æ”¯æŒç›´æ¥åŠ è½½ 8-bitï¼Œå·²è‡ªåŠ¨åˆ‡æ¢åˆ°æ‰‹åŠ¨é‡åŒ–æ–¹å¼")
            raise e
        except Exception as e:
            print(f"âŒ è½¬å½•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            raise e
        finally:
            # 9. æ¸…ç†ä¸´æ—¶æ–‡ä»¶ (ç¡®ä¿æ— è®ºæ˜¯å¦å‡ºé”™éƒ½æ‰§è¡Œ)
            if temp_audio_path and os.path.exists(temp_audio_path):
                try:
                    os.unlink(temp_audio_path)
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {temp_audio_path}: {e}")

    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹è¯¦ç»†ä¿¡æ¯"""
        info = {
            "mode": self.mode,
            "device": str(self.device),
            "model_dtype": str(self.model_dtype),
            "target_sampling_rate": self.target_sr,
            "checkpoint_dir": str(self.checkpoint_dir),
            "is_glm_asr": self.is_glm_asr,
        }
        
        if self.device.type == "cuda":
            info.update({
                "cuda_version": torch.version.cuda,
                "gpu_name": torch.cuda.get_device_name(),
                "gpu_memory_total_mb": torch.cuda.get_device_properties(0).total_memory / 1024**2,
            })
        
        return info


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹1ï¼šåŸç”Ÿæ¨¡å¼ï¼ˆé€‚åˆå¤§æ˜¾å­˜æ˜¾å¡ï¼‰
    try:
        print("\n=== æµ‹è¯•åŸç”Ÿæ¨¡å¼ ===")
        asr_native = ASRModel(
            checkpoint_dir="./glm-asr-model",
            device="cuda",
            mode="native"  # åŸç”Ÿ bfloat16 æ¨¡å¼
        )
        print("âœ… åŸç”Ÿæ¨¡å¼æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        print(f"æ¨¡å‹ä¿¡æ¯: {asr_native.get_model_info()}")
    except Exception as e:
        print(f"âŒ åŸç”Ÿæ¨¡å¼åˆå§‹åŒ–å¤±è´¥: {e}")

    # ç¤ºä¾‹2ï¼šINT8 æ¨¡å¼ï¼ˆé€‚åˆ GTX1060 ç­‰å°æ˜¾å­˜æ˜¾å¡ï¼‰
    try:
        print("\n=== æµ‹è¯• INT8 æ¨¡å¼ ===")
        asr_int8 = ASRModel(
            checkpoint_dir="./glm-asr-model",
            device="cuda", 
            mode="int8"  # 8-bit é‡åŒ–æ¨¡å¼
        )
        print("âœ… INT8 æ¨¡å¼æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        print(f"æ¨¡å‹ä¿¡æ¯: {asr_int8.get_model_info()}")
    except Exception as e:
        print(f"âŒ INT8 æ¨¡å¼åˆå§‹åŒ–å¤±è´¥: {e}")
        print("ğŸ’¡ æç¤º: ç¡®ä¿å·²å®‰è£… bitsandbytes: pip install bitsandbytes")

    # ç¤ºä¾‹3ï¼šCPU æ¨¡å¼ï¼ˆæ—  GPU æ—¶ï¼‰
    try:
        print("\n=== æµ‹è¯• CPU æ¨¡å¼ ===")
        asr_cpu = ASRModel(
            checkpoint_dir="./glm-asr-model",
            device="cpu",
            mode="native"  # CPU ä¸æ”¯æŒ INT8
        )
        print("âœ… CPU æ¨¡å¼æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        print(f"æ¨¡å‹ä¿¡æ¯: {asr_cpu.get_model_info()}")
    except Exception as e:
        print(f"âŒ CPU æ¨¡å¼åˆå§‹åŒ–å¤±è´¥: {e}")